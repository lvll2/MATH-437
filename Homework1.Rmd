---
title: 'Homework Assignment #1'
author: "Math 437 - Modern Data Analysis"
date: "Due February 10, 2023"
output: pdf_document
---

# Instructions

You should submit either two or three files:

1. You should write your solutions to the Simulation and Applied Problems in this R Markdown file and submit the (.Rmd) file.
2. You should knit the final solution file to pdf and submit the pdf. If you are having trouble getting code chunks to run, add `eval = FALSE` to the chunks that do not run. If you are having trouble getting R Studio to play nice with your LaTeX distribution, I will begrudgingly accept an HTML file instead. 
3. Solutions to the Key Terms and Conceptual Problems can be submitted in a separate Word or pdf file or included in the same files as your solutions to the Simulation and Applied Problems.

This homework assignment is worth a total of **40 points**.

# Key Terms (5 pts)

Read Chapter 2 of Introduction to Statistical Learning, Second Edition. Based on your reading, answer the following questions.

1. What is the difference between an *input variable* and an *output variable* in a model? Provide synonyms for each term.

Answer: An input variable is typically our X, or predictors, there can be multiple, 
whereas the output variable is denoted as Y and is the response variable. The 
reading uses the idea of sales being Y, our output, and variables such as tv and radio 
budget being X, or input variables. 

2. What is the difference between *reducible error* and *irreducible error*? Give an example (other than those given in the book) of a situation in which the irreducible error is greater than zero.

Answer: Reducible error comes from inaccuracy in the estimate that can be reduced by using the appropriate statistical technique and irreducible error comes during the data collection process because of influential but unmeasured variables and variability in the collected data. Irreducible error cannot be eliminated. An example would be in a study about teenager growth patterns in America, their nutrition may not be recorded exactly which would be a variable influencing their growth and therefore would introduce irreducible error.

3. Generally, what types of questions are answered using *inference* and what types are answered using *prediction*? Is it possible to use the same model for both inference and prediction?

Answer: In general, questions that would be answered in inference would include trying to understand the deeper relationship between our predictors, x, and our response, y, whereas in prediction we are less concerned with the relationship but instead how the predictors, x, lead to the outcome, y. The reading uses the example as inference asks the relationship between x and y and prediction asks what values of x leads to y. Also, it is possible to use the same model for both inference and prediction, just depending on how one interprets as well as the complexity of the model used.

4. Generally, what types of prediction questions are answered using *regression* methods and what types are answered using *classification* methods?

Answer: The types of prediction questions answered using regression methods are those that have a quantitative response and prediction questions with qualitative responses are generally answered using classification methods. Although some methods can be used for either response type.

5. What are the major advantages of using a *nonparametric* method over a *parametric* method? What are the disadvantages?

Answer: The major advantages of nonparametric methods is the fact that the problem is reduced to simply finding the parameters instead of the function itself, this is great when we are able to assume the function has a form we can understand such as a linear form but when the form of the data is more complex, our linear model could become a super complicated 13th Taylor polynomial that breaks when one more piece of data is added(over fitting). When the form of the data is complex a nonparametric approach is generally better to use.

6. In prediction, we typically aim to minimize a *loss function* that more-or-less represents the total error in our predictions. Give one example each for regression and classification problems of a measure of model (in)accuracy.

Answer: A measure of model accuracy for regression problems is mean squared error and for classification problems the most common measure used is error rate. In both cases, the MSE and error rate should be computed using test data rather than data from the training set.

7. Why do we only fit the model on a *training set*? What do we do with the rest of the data?

Answer: We only fit the model of a training set as it has the majority of the data. This allows us to test the rest of the data into the model to determine how well the model is doing using real collected data.

8. Generally, as a model becomes more complex, what happens to the *bias* of the model and why? What happens to the *variance* of the model and why?

Answer: As a model becomes more complex, or flexible, the bias of the model will decrease as it will more closely follow the training data which often has non-linear relationships. However, as the model becomes more complex, it will have higher variance as it follows the training data because its flexibility makes the model sensitive to changes in the training data that would not have a major effect on more simplified, rigid models.

9. What is meant by the term *overfitting*? Explain this in terms of the bias-variance trade-off.

Answer: Overfitting is when too much power is given to the training data set that inevitably leads to large variance when cross validating. The best way to see this is when the training dataset changes considerably as a result from new or different data values being used when modeling. This can also be seen in the bias when working with real life examples of the data in which our error rate becomes increasingly large implying that the data is not following a trend but instead our model is solely basing its' values on that of the training data (Very clear when cross validating to determine whether or not we are overfitting).

10. Briefly explain how a *Bayes classifier* works.

Answer: 

# Conceptual Problems

## Conceptual Problem 1 (4 pts)

Write me a brief (2-3 paragraphs) summary of what you learned in the P-Values and Power in-class activity about how the distribution of p-values (over very many tests) is affected by the validity/violation of test assumptions and the power of the test. Did anything surprise you or clarify a concept for you? Support your writing with a few graphs you produced in class (it is easiest to copy and re-run the relevant code chunks).

The P-values and Power in class activity clarified a few topics. The primary takeaway was despite the population distribution not being normal, the spread of p-values are which led to us seeing 

## Conceptual Problem 2 (3 pts) 

Textbook Exercise 2.4.4 

4. You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.
Answer:

(b) Describe three real-life applications in which regression might
be useful. Describe the response, as well as the predictors. Is the
goal of each application inference or prediction? Explain your
answer.
Answer: 

(c) Describe three real-life applications in which cluster analysis
might be useful.
Answer: 

## Conceptual Problem 3 (3 pts) 

Textbook Exercise 13.7.2

# Simulation Problems

## Simulation Problem 1 (Code: 4 pts; Explanation: 6 pts)

From the Parametric vs. Nonparametric Tests: Two-Sample Tests activity, copy to this homework your simulation code/results from the *Assumptions Violated, Ha True* section of each test as well as the results tables for all simulations (in the Class Results section). 

_______________________________________________________________________________
t.test section:
```{r p-values2 example, eval = FALSE}
nG <- 10
d <- 0.8
for (i in 1:length(pvalues)){
  set.seed(i)  # notice that the seed changes every time inside the for loop
  # you could also set a single seed outside the for loop
  
  # Create the vectors x and y
  x <- c(rnorm(nG*0.9, mean = 0, sd = sqrt(0.19)),
         rnorm(nG*0.1, mean = 3, sd = sqrt(0.19))
)
  y <- c(rnorm(nG*0.9, mean = d, sd = sqrt(0.19)),
         rnorm(nG*0.1, mean = 3 + d, sd = sqrt(0.19))
)

  # Perform the t-test and get the p-value
  pvalues[i] <- t.test(x, y, alternative = "t")$p.value
    
}
```

```{r p-values 3 example, eval = FALSE}
# histogram of the p-values under H0
hist(pvalues)

plot(ecdf(pvalues),
     xlab = "p-value",
     ylab = "F(p-value)",
     main = "Empirical CDF of the P-Value Under H0")

mean(pvalues <= 0.05)

```
Mann-Whitney Section: 
```{r p-values2 example, eval = FALSE}
nG <- 10
d <- 0.8
for (i in 1:length(pvalues)){
  set.seed(i)  # notice that the seed changes every time inside the for loop
  # you could also set a single seed outside the for loop
  
  # Create the vectors x and y
  x <- c(rnorm(nG*0.9, mean = 0, sd = sqrt(0.19)),
         rnorm(nG*0.1, mean = 3, sd = sqrt(0.19))
)
  y <- c(rnorm(nG*0.9, mean = d, sd = sqrt(0.19)),
         rnorm(nG*0.1, mean = 3 + d, sd = sqrt(0.19))
)

  # Perform the t-test and get the p-value
  pvalues[i] <- wilcox.test(x, y, alternative = "t")$p.value
    
}
```

```{r p-values 3 example, eval = FALSE}
# histogram of the p-values under H0
hist(pvalues)

plot(ecdf(pvalues),
     xlab = "p-value",
     ylab = "F(p-value)",
     main = "Empirical CDF of the P-Value Under H0")

mean(pvalues <= 0.05)

```
_______________________________________________________________________________

From the Parametrics vs. Nonparametric Tests: Multi-Sample Tests activity, copy to this homework your simulation code/results from the *Both Assumptions Violated* section of each test as well as the results tables for all simulations (in the Class Results section).

_______________________________________________________________________________

_______________________________________________________________________________

Write a couple of paragraphs explaining the difference between parametric and nonparametric methods and explain why classic nonparametric methods (Mann-Whitney and Kruskal-Wallis) are a better choice than the corresponding parametric methods (two-sample t-test and one-way ANOVA) when the assumptions of the parametric method are clearly violated.

Answer: The primary reason why we would use a non-parametric model is when we violate our assumptions. It was clear that violated assumptions destroyed the power and validity of the tests that we were performing but when using non-parametric methods those violations were not of concern. This ties into the idea of not having to assume a family of functions that the data would follow but instead we tested based the shape of the data as it was.

# Applied Problems

## Applied Problem 1 (Code: 6 pts; Explanation: 3 pts)

Textbook Exercise 2.4.8 with the following changes:

* Use the `College` dataset already in the `ISLR2` package instead of doing parts (a) and (b).
* Replace the four lines of code in part (c.iv) with a single line that accomplishes the same thing, using the `mutate` and either `if_else` or `case_when` functions from the `dplyr` package.
* As part of your brief summary in part (c.vi), identify at least one data point that cannot possibly have been recorded correctly, and explain why.

```{r}
  
# Elite <- rep ("No", nrow (college))
# Elite[college$Top10perc > 50] <- "Yes "
# Elite <- as.factor (Elite)
# college <- data.frame (college , Elite)

college %>% mutate(Elite = case_when(Top10perc > 50 ~"Yes",Top10perc<= 50 ~ "No"))




```


## Applied Problem 2 (Code: 1 pt; Explanation: 2 pts)

Molitor (1989) hypothesized that children who watched violent film and television were more tolerant of violent "real-life" behavior. A sample of 42 children were randomly assigned to watch footage from either the 1984 Summer Olympics (non-violent) or the movie \emph{The Karate Kid} (violent). They were then told to watch (by video monitor) two younger children in the next room and get the research assistant if they "got into trouble" (the monitor actually showed a pre-recorded video of the children getting progressively more violent).

The file \emph{violence.csv} contains the time (in seconds) that each child stayed in the room. Longer stays are assumed to indicate more tolerance of violent behavior. Produce an appropriate graph showing the sample data and, based on your graph, explain why a two-sample t-test might not be the best idea.

```{r}
violent_data <- read.csv("violence.csv")

ggplot(data = violent_data ,aes(x = Video, y= Time)) +
  geom_boxplot()+
  geom_point()

ggplot(data = violent_data,
       mapping = aes(x = Time)) +
  geom_histogram() +
  facet_wrap(~Video)

violent_data %>% 
  filter(Video == "Olympics") %>% 
  summary()
violent_data %>% 
  filter(Video == "Karate Kid") %>% 
  summary()

```




## Applied Problem 3 (Code: 1 pt; Explanation: 2 pts)

Use the permutation test function you wrote in Lab 2 to determine whether the research hypothesis in the previous question was supported. Be sure to follow all steps of hypothesis testing, up to and including writing a conclusion that answers the research question in context.
