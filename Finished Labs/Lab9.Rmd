---
title: 'Lab Assignment #9'
author: "Nick Noel, Liz Villa, & Cadee Pinkerton"
date: "Due April 12, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce ridge regression and LASSO.

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, include = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(glmnet)
library(insuranceData)
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Book Code

## Part a (Code: 2 pts)
Run the code in ISLR Lab 6.5.2.

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
x <- model.matrix(Salary ~ ., Hitters)[,-1]

y <- Hitters$Salary
```

```{r}
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x,y,alpha = 0, lambda = grid)
```

```{r}
dim(coef(ridge.mod))
```

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

```{r}
predict(ridge.mod, s=50, type = "coefficients")[1:20,]
```

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx = x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
mean((mean(y[train])-y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s=1e10, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s=0, newx = x[test,], exact = T, x = x[train,], y = y[train])
mean((ridge.pred - y.test)^2)

lm(y~x, subset = train)
predict(ridge.mod, s=0, exact = T, type = "coefficients", x = x[train, ], y = y[train])[1:20,]
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r}
ridge.pred <- predict(ridge.mod, s= bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)
```

```{r}
out <- glmnet(x,y,alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20,]
```

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
```

```{r}
out <- glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

## Part b (Explanation: 1 pt)

Which variables in the original Hitters dataset are *not* in the matrix `x`? What happened to them?

The response variable, Salary, is not in the matrix `x`. The categorical variables have been converted to dummy variables.

## Part c (Explanation: 1 pt)

Explain how to "force" R to fit an intercept-only model using the `glmnet` function.

We can force R to fit an intercept-only model by setting the argument `lambda` equal to a very large number, for example $10^{10}$.

## Part d (Explanation: 1 pt)

Given an optimal cross-validated value of $\lambda$, does ridge regression or LASSO tend to produce simpler models? Explain your reasoning.

LASSO tends to produce simpler models since it performs variable selection based on the optimal value of $\lambda$.


# Problem 2: Auto Insurance Claims

The `AutoBi` dataset in the `insuranceData` package contains information about a sample of 1,340 automobile insurance claims from 2002.

The response variable here is `LOSS`, the total economic loss (in thousands). We take the base 10 log of the claimed loss to make the regression a bit easier, and transform several predictors, to create the `AutoBi2` dataset.

```{r insurance fix}
data(AutoBi) # Auto insurance claim dataset

AutoBi2 <- AutoBi %>% transmute(
  attorney = if_else(ATTORNEY == 1, "yes", "no"),
  gender = if_else(CLMSEX == 1, "male", "female"),
  marital = case_when(MARITAL == 1 ~ "married",
                      MARITAL == 2 ~ "single",
                      MARITAL == 3 ~ "widowed",
                      MARITAL == 4 ~ "divorced",
                      TRUE ~ NA_character_),
  driver_insured = if_else(CLMINSUR == 1, "yes", "no"),
  seatbelt = if_else(SEATBELT == 1, "yes", "no"),
  age = CLMAGE,
  log_loss = log10(1000*LOSS) # log 10 of claimed loss in the claim, not actually log-loss as in the accuracy metric
) %>% filter(!is.na(gender), !is.na(marital), !is.na(driver_insured),
                              !is.na(seatbelt), !is.na(age))
```

## Part a (Code: 1 pt)

Divide the `AutoBi2` dataset into a training set and a test set. The test set should contain approximately 25% of the original dataset.

```{r}
# with tidymodels
set.seed(1880)
auto_split <- initial_split(AutoBi2, prop = 0.75) 

auto_train <- training(auto_split)
auto_test <- testing(auto_split)
```


## Part b (Code: 2 pts)

Using tidymodels, set up a workflow for a ridge regression model predicting `log_loss` from the other variables (`attorney`, `gender`, `marital`, `driver_insured`, `seatbelt`, and `age`) including both the appropriate `model` and `recipe`.  In part (c) you will tune the model, so make sure to include `penalty = tune()`.

```{r}
ridge_model <- linear_reg(mode = "regression", engine = "glmnet",
                          penalty = tune(), # let's tune the lambda penalty term
                          mixture = 0) # mixture = 0 specifies pure ridge regression

ridge_wflow <- workflow() %>%
  add_model(ridge_model)

ridge_recipe <- recipe(
  log_loss ~ attorney + gender + marital + driver_insured + seatbelt + age, # response ~ predictors
  data = auto_train
) %>%
  step_normalize(all_numeric_predictors()) %>% # don't scale the response
  step_dummy(all_nominal_predictors())

ridge_wflow <- ridge_wflow %>%
  add_recipe(ridge_recipe)
```


## Part c (Code: 2 pts)

Use 10-fold cross-validation with 2 repeats to determine the optimal value of $\lambda$ using the 1-standard error rule and the RMSE metric. It turns out that not a lot of shrinkage is necessary here; use a grid from 0 to 0.5 in increments of 0.05 (i.e., use the `expand.grid` function to manually set up your $\lambda$ grid rather than using `grid_regular`).
```{r}
set.seed(1332)
auto_kfold <- vfold_cv(auto_train, v = 10, repeats = 2) 

ridge_grid <- expand.grid(penalty = seq(0, 0.5, by = 0.05))

ridge_tune1 <- tune_grid(ridge_model, 
                      ridge_recipe, 
                      resamples = auto_kfold, 
                      grid = ridge_grid)

ridge_best <- select_by_one_std_err(
  ridge_tune1,
  metric = "rmse",
  desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest
)
ridge_best
```

What is the optimal value of $\lambda$ according to your cross-validation?

Th optimal value is $\lambda = 1.58$.

## Part d (Code: 2 pts)

Finalize your ridge regression workflow to use this value of $\lambda$, then fit the ridge regression model on the entire training set.

Use the model to make predictions on the test set and obtain the estimate of test RMSE. 

```{r fit ridge-tidy model}
ridge_wflow_final <- finalize_workflow(ridge_wflow, parameters = ridge_best) 

ridge_fit <- fit(ridge_wflow_final, data = auto_train)
ridge_fit
```

```{r augment ridge fit}
predictions_ridge <- broom::augment(ridge_fit, new_data = auto_test)
predictions_ridge %>% dplyr::select(log_loss, .pred
)
rmse(predictions_ridge, truth = log_loss, estimate = .pred)
```

```{r ridge plot check}
ggplot(predictions_ridge, aes(x = .pred, y = log_loss)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "blue")
```

## Part e (Code: 3 pts)

Repeat parts (b)-(d) for the LASSO model.

```{r Lasso-tidy model}
lasso_model <- linear_reg(mode = "regression", engine = "glmnet",
                          penalty = tune(), # let's tune the lambda penalty term
                          mixture = 1) # mixture = 1 specifies pure LASSO

lasso_wflow <- workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(ridge_recipe) # same recipe is needed, no need to reinvent the wheel
```

```{r tune model kfold lasso}
lasso_tune1 <- tune_grid(lasso_model, 
                      ridge_recipe, 
                      resamples = auto_kfold, 
                      grid = ridge_grid)
```

```{r select best lasso}
lasso_best <- select_by_one_std_err(
  lasso_tune1,
  metric = "rmse",
  desc(penalty) # order penalty from largest (highest bias = simplest model) to smallest
)
lasso_best
```

```{r fit lasso-tidy model}
lasso_wflow_final <- finalize_workflow(lasso_wflow, parameters = lasso_best) 

lasso_fit <- fit(lasso_wflow_final, data = auto_train)
lasso_fit
```

```{r augment lasso fit}
predictions_lasso <- broom::augment(lasso_fit, new_data = auto_test)
predictions_lasso %>% dplyr::select(log_loss, .pred
)
rmse(predictions_lasso, truth = log_loss, estimate = .pred)
```

```{r lasso plot check}
ggplot(predictions_lasso, aes(x = .pred, y = log_loss)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "blue")
```
