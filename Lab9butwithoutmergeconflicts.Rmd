---
title: 'Lab Assignment #9'
author: "Math 437 - Modern Data Analysis"
date: "Due April 12, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce ridge regression and LASSO.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(glmnet)
library(insuranceData)
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Book Code

## Part a (Code: 2 pts)

Run the code in ISLR Lab 6.5.2.

## Part b (Explanation: 1 pt)

Which variables in the original Hitters dataset are *not* in the matrix `x`? What happened to them?

The response variable, Salary, is not in the matrix `x`. The categorical variables have been converted to dummy variables.

## Part c (Explanation: 1 pt)

Explain how to "force" R to fit an intercept-only model using the `glmnet` function.

We can force R to fit an intercept-only model by setting the argument `lambda` equal to a very large number, for example $10^{10}$.

## Part d (Explanation: 1 pt)

Given an optimal cross-validated value of $\lambda$, does ridge regression or LASSO tend to produce simpler models? Explain your reasoning.

LASSO tends to produce simpler models because of its variable selection 
# Problem 2: Auto Insurance Claims

The `AutoBi` dataset in the `insuranceData` package contains information about a sample of 1,340 automobile insurance claims from 2002.

The response variable here is `LOSS`, the total economic loss (in thousands). We take the base 10 log of the claimed loss to make the regression a bit easier, and transform several predictors, to create the `AutoBi2` dataset.

```{r insurance fix}
data(AutoBi) # Auto insurance claim dataset

AutoBi2 <- AutoBi %>% transmute(
  attorney = if_else(ATTORNEY == 1, "yes", "no"),
  gender = if_else(CLMSEX == 1, "male", "female"),
  marital = case_when(MARITAL == 1 ~ "married",
                      MARITAL == 2 ~ "single",
                      MARITAL == 3 ~ "widowed",
                      MARITAL == 4 ~ "divorced",
                      TRUE ~ NA_character_),
  driver_insured = if_else(CLMINSUR == 1, "yes", "no"),
  seatbelt = if_else(SEATBELT == 1, "yes", "no"),
  age = CLMAGE,
  log_loss = log10(1000*LOSS) # log 10 of claimed loss in the claim, not actually log-loss as in the accuracy metric
) %>% filter(!is.na(gender), !is.na(marital), !is.na(driver_insured),
                              !is.na(seatbelt), !is.na(age))
```

## Part a (Code: 1 pt)

Divide the `AutoBi2` dataset into a training set and a test set. The test set should contain approximately 25% of the original dataset.

## Part b (Code: 2 pts)

Using tidymodels, set up a workflow for a ridge regression model predicting `log_loss` from the other variables (`attorney`, `gender`, `marital`, `driver_insured`, `seatbelt`, and `age`) including both the appropriate `model` and `recipe`.  In part (c) you will tune the model, so make sure to include `penalty = tune()`.

## Part c (Code: 2 pts)

Use 10-fold cross-validation with 2 repeats to determine the optimal value of $\lambda$ using the 1-standard error rule and the RMSE metric. It turns out that not a lot of shrinkage is necessary here; use a grid from 0 to 0.5 in increments of 0.05 (i.e., use the `expand.grid` function to manually set up your $\lambda$ grid rather than using `grid_regular`).

What is the optimal value of $\lambda$ according to your cross-validation?

## Part d (Code: 2 pts)

Finalize your ridge regression workflow to use this value of $\lambda$, then fit the ridge regression model on the entire training set.

Use the model to make predictions on the test set and obtain the estimate of test RMSE. 

## Part e (Code: 3 pts)

Repeat parts (b)-(d) for the LASSO model.
