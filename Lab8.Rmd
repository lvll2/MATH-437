---
title: 'Lab Assignment #8'
author: "Math 437 - Modern Data Analysis"
date: "Due April 5, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce model selection in regression.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(ggplot2)
library(dplyr)
library(leaps)

madden17_QB <- readr::read_csv("madden17_QB.csv")
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Model Selection Using Cross-Validation

## Part a (Code: 1 pt)

Run the code in ISLR Labs 5.3.2 and 5.3.3. Put each chunk from the textbook in its own chunk.

```{r}
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
```

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
```

```{r warning = FALSE}
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

```{r}
cv.error <- rep(0, 10)
for(i in 1:10){
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

```{r}
set.seed(17)
cv.error.10 <- rep(0, 10)
for(i in 1:10){
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}

cv.error.10
```


## Part b (Code: 3 pts; Explanation: 1 pt)

Using the Auto dataset and 5-fold cross-validation, determine which of these sets of predictors produces the best linear model for predicting mpg, and explain your reasoning:

* horsepower and displacement
* acceleration and displacement
* horsepower and acceleration
* horsepower, acceleration, and displacement

*Do not* use the `cv.glm` function. Instead, modify the code in the "Automated Model Selection" class activity to do the cross-validation. Either the "Base R" or "tidymodels" example is fine to follow.

```{r}
set.seed(17)

k <- 5

reorder_rows <- sample(nrow(Auto))

fold_numbers <- (reorder_rows %% k) + 1
```

```{r}
model_MSE <- function(model, df, response){
  # model: a model object
  # df: a data frame on which we want to predict
  # response: a character vector giving the name of the response variable
  
  predictions <- predict(model, newdata = df)
  MSE <- mean((predictions - df[[response]])^2)
  return(MSE)
}
```

```{r}
models <- vector("list", length = 4)
models[[1]] <- lm(mpg ~ horsepower + displacement, data = Auto)
models[[2]] <- lm(mpg ~ acceleration + displacement, data = Auto)
models[[3]] <- lm(mpg ~ horsepower + acceleration, data = Auto)
models[[4]] <- lm(mpg ~ horsepower + acceleration + displacement, data = Auto)

```

```{r}
nmodels <- length(models)
cv_error <- matrix(0, nrow = k, ncol = nmodels)
# each row of cv_error represents a fold
# each column of cv_error represents a model

for (i in 1:k){
  fold_validation_rows <- which(fold_numbers == i)
  train_set <- Auto[-fold_validation_rows,]
  validation_set <- Auto[fold_validation_rows,]
  
  for(j in 1:nmodels){
    models[[j]] <- update(models[[j]], data = train_set)
    cv_error[i, j] <- model_MSE(models[[j]], df = validation_set, response = "mpg")
  }
}

```

```{r}
cv_rmse <- sqrt(cv_error)
apply(cv_rmse, 2, mean)
```

The best model based on lowest rmse is the model with all three predictors.

## Part c (Code: 2 pts)

For the model you selected in part (b), re-fit the model on the entire `Auto` dataset. Then, write a couple of lines of code to compute $C_p$ and $BIC$ for this model (as given in the book) without relying on the `AIC`/`BIC` functions or any functions in the `olsrr` package. Some hints:

* You can obtain RSS by creating an `aov` object and running the code `summary(aov_object)[[1]]`, then finding the appropriate way to subset the resulting matrix.
* You can obtain $\hat{\sigma}$ for a model by running `summary(full_model)$sigma`. You may assume that the full model is the one with all three predictors.

```{r}
aov_object <- aov(lm4)

summary(aov_object)[[1]]

RSS <- summary(aov_object)[[1]][4, 2]
```

```{r}
sig_hat <- summary(lm4)$sigma
```

```{r}
(Cp <- 1/nrow(Auto)*(RSS + 2*3*sig_hat^2))

(BIC <- 1/nrow(Auto)*(RSS + log(nrow(Auto))*3*sig_hat^2))
```


## Problem 2: Subset Selection

### Part a (Code: 1 pt)

Run the code in ISLR Lab 6.5.1, "Best Subset Selection" and "Forward and Backward Stepwise Selection" subsections. (Do not run the "Choosing Among Models Using the Validation-Set Approach and Cross-Validation" section.)

### Part b (Explanation: 1 pt)

Briefly explain how to interpret the plots created by `plot(regfit.full, scale = "some metric")` at the end of the Best Subset Selection section.

### Part c (Code: 1 pt; Explanation: 1 pt)

In the rest of this problem, we will explore a situation in which the true model is *known* (more-or-less). In this true model, however, the error term is due to rounding and is *not* normally distributed, and there are some major collinearity issues. Let's see whether these violations of least-squares assumptions affect subset selection.

The madden17_QB dataset contains the overall rating (`OVR`) and individual skill ratings for 112 quarterbacks in the Madden NFL 2017 video game. According to an article on fivethirtyeight.com, the overall rating for quarterbacks is a linear combination of the following skill ratings: `AWR`, `THP`, `SAC`, `MAC`, `DAC`, `PAC`, `SPD`, `AGI`, `RUN`, and `ACC`. The other 34 skill ratings are not relevant.

Perform best subset selection on this dataset, using `nvmax = 10`. You may have to remove the categorical variables (`Name` and `Team`) in the formula or the dataset used to fit the model.

Did the algorithm correctly identify the 10 important variables in the model? If not, which variables were incorrectly left out, and which were incorrectly included?

### Part d (Code: 1 pt; Explanation: 1 pt)

Perform forward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?

## Part e (Code: 1 pt; Explanation: 1 pt)

Perform backward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?
