---
title: 'Lab Assignment #8'
author: "Nick Noel and Liz Villa"
date: "Due April 5, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce model selection in regression.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(tidyverse)
library(ISLR2)
library(ggplot2)
library(dplyr)
library(leaps)

madden17_QB <- readr::read_csv("madden17_QB.csv")
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Model Selection Using Cross-Validation

## Part a (Code: 1 pt)

Run the code in ISLR Labs 5.3.2 and 5.3.3. Put each chunk from the textbook in its own chunk.

```{r}
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
```

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
```

```{r warning = FALSE}
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

```{r}
cv.error <- rep(0, 10)
for(i in 1:10){
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

```{r}
set.seed(17)
cv.error.10 <- rep(0, 10)
for(i in 1:10){
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}

cv.error.10
```


## Part b (Code: 3 pts; Explanation: 1 pt)

Using the Auto dataset and 5-fold cross-validation, determine which of these sets of predictors produces the best linear model for predicting mpg, and explain your reasoning:

* horsepower and displacement
* acceleration and displacement
* horsepower and acceleration
* horsepower, acceleration, and displacement

*Do not* use the `cv.glm` function. Instead, modify the code in the "Automated Model Selection" class activity to do the cross-validation. Either the "Base R" or "tidymodels" example is fine to follow.

```{r}
set.seed(17)

k <- 5

reorder_rows <- sample(nrow(Auto))

fold_numbers <- (reorder_rows %% k) + 1
```

```{r}
model_MSE <- function(model, df, response){
  # model: a model object
  # df: a data frame on which we want to predict
  # response: a character vector giving the name of the response variable
  
  predictions <- predict(model, newdata = df)
  MSE <- mean((predictions - df[[response]])^2)
  return(MSE)
}
```

```{r}
models <- vector("list", length = 4)
models[[1]] <- lm(mpg ~ horsepower + displacement, data = Auto)
models[[2]] <- lm(mpg ~ acceleration + displacement, data = Auto)
models[[3]] <- lm(mpg ~ horsepower + acceleration, data = Auto)
models[[4]] <- lm(mpg ~ horsepower + acceleration + displacement, data = Auto)

```

```{r}
nmodels <- length(models)
cv_error <- matrix(0, nrow = k, ncol = nmodels)
# each row of cv_error represents a fold
# each column of cv_error represents a model

for (i in 1:k){
  fold_validation_rows <- which(fold_numbers == i)
  train_set <- Auto[-fold_validation_rows,]
  validation_set <- Auto[fold_validation_rows,]
  
  for(j in 1:nmodels){
    models[[j]] <- update(models[[j]], data = train_set)
    cv_error[i, j] <- model_MSE(models[[j]], df = validation_set, response = "mpg")
  }
}

```

```{r}
cv_rmse <- sqrt(cv_error)
apply(cv_rmse, 2, mean)
```

The best model based on lowest rmse is the model with all three predictors.

## Part c (Code: 2 pts)

For the model you selected in part (b), re-fit the model on the entire `Auto` dataset. Then, write a couple of lines of code to compute $C_p$ and $BIC$ for this model (as given in the book) without relying on the `AIC`/`BIC` functions or any functions in the `olsrr` package. Some hints:

* You can obtain RSS by creating an `aov` object and running the code `summary(aov_object)[[1]]`, then finding the appropriate way to subset the resulting matrix.
* You can obtain $\hat{\sigma}$ for a model by running `summary(full_model)$sigma`. You may assume that the full model is the one with all three predictors.

```{r}
lm4 <- lm(mpg ~ horsepower + acceleration + displacement, data = Auto)

aov_object <- aov(lm4)

summary(aov_object)[[1]]

RSS <- summary(aov_object)[[1]][4, 2]
```

```{r}
sig_hat <- summary(lm4)$sigma
```

```{r}
(Cp <- 1/nrow(Auto)*(RSS + 2*3*sig_hat^2))

(BIC <- 1/nrow(Auto)*(RSS + log(nrow(Auto))*3*sig_hat^2))
```


## Problem 2: Subset Selection

### Part a (Code: 1 pt)

Run the code in ISLR Lab 6.5.1, "Best Subset Selection" and "Forward and Backward Stepwise Selection" subsections. (Do not run the "Choosing Among Models Using the Validation-Set Approach and Cross-Validation" section.)

```{r}
library(ISLR2)
names(Hitters)

dim(Hitters)
sum(is.na(Hitters$Salary))
```

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
```

```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
```

```{r}
names(reg.summary)
```

```{r}
reg.summary$rsq
```

```{r}
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
```

```{r eval=FALSE, include=FALSE}
which.max(reg.summary$adjr2)
```


```{r eval=FALSE, include=FALSE}
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch =20)
```

```{r}
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex = 2, pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "bic", type = "l" )
points(6, reg.summary$bic[6],col="red", cex = 2, pch = 20 )
```


```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```

```{r}
coef(regfit.full,6)
```


# Forward and Backward Stepwise Selection

```{r}
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <-  regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
```

```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```


### Part b (Explanation: 1 pt)

Briefly explain how to interpret the plots created by `plot(regfit.full, scale = "some metric")` at the end of the Best Subset Selection section.

To interpret the plots you are looking for either the highest ($R^2$ and $adjustedR^2$) or lowest (Cp and BIC) values. These will be colored black on the plot which tells us what predictors are the most useful in terms of prediction. Thus for BIC the best model will be the one with the 6 variables that are at the top of the plot and colored black.

### Part c (Code: 1 pt; Explanation: 1 pt)

In the rest of this problem, we will explore a situation in which the true model is *known* (more-or-less). In this true model, however, the error term is due to rounding and is *not* normally distributed, and there are some major collinearity issues. Let's see whether these violations of least-squares assumptions affect subset selection.

The madden17_QB dataset contains the overall rating (`OVR`) and individual skill ratings for 112 quarterbacks in the Madden NFL 2017 video game. According to an article on fivethirtyeight.com, the overall rating for quarterbacks is a linear combination of the following skill ratings: `AWR`, `THP`, `SAC`, `MAC`, `DAC`, `PAC`, `SPD`, `AGI`, `RUN`, and `ACC`. The other 34 skill ratings are not relevant.

Perform best subset selection on this dataset, using `nvmax = 10`. You may have to remove the categorical variables (`Name` and `Team`) in the formula or the dataset used to fit the model.

```{r}
the_big_game <- madden17_QB %>%
  select(-"Name", -"Team")

big_game_full_model <- regsubsets(OVR ~ ., data = the_big_game, nvmax = 10)
big_game_summary <- summary(big_game_full_model)
plot(big_game_full_model, scale = "bic")

coef(big_game_full_model,10)

```

Did the algorithm correctly identify the 10 important variables in the model? If not, which variables were incorrectly left out, and which were incorrectly included?

SFA from literature was not significant but was chosen in our model, the algorithm also removed the variable we expected to be significant, ACC.

### Part d (Code: 1 pt; Explanation: 1 pt)

Perform forward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?

```{r}
the_big_game.fwd <- regsubsets(OVR~., data = the_big_game, nvmax = 20, method = "forward")
big_summary_fwd <- summary(the_big_game.fwd)
plot(big_summary_fwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(big_summary_fwd$cp)
points(15, big_summary_fwd$cp[15], col="red", cex = 2, pch = 20)

coef(the_big_game.fwd, 15)

plot(big_summary_fwd$bic, xlab = "Number of Variables", ylab = "bic", type = "l")
which.min(big_summary_fwd$bic)
points(12, big_summary_fwd$bic[12], col="red", cex = 2, pch = 20)

coef(the_big_game.fwd, 12)
```
BIC had 12 significant variables and cp had 15 variables. Both BIC and cp had all 10 variables that previous literature mentioned but BIC also incorrectly included SFA and PBK and cp also incorrectly included SFA, CTH, SPC, RLS, PBK.

## Part e (Code: 1 pt; Explanation: 1 pt)

Perform backward selection on this dataset, using `nvmax = 20`. How many variables are in the "best" model using BIC as a selection criterion? What about Cp? For the "best" model (using one of the criteria), which variables were incorrectly left out, and which were incorrectly included?

```{r}
the_big_game.bwd <- regsubsets(OVR~., data = the_big_game, nvmax = 20, method = "backward")
big_summary_bwd <- summary(the_big_game.bwd)
plot(big_summary_bwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(big_summary_bwd$cp)
points(19, big_summary_bwd$cp[19], col="red", cex = 2, pch = 20)

coef(the_big_game.bwd, 19)

plot(big_summary_bwd$bic, xlab = "Number of Variables", ylab = "bic", type = "l")
which.min(big_summary_bwd$bic)
points(10, big_summary_bwd$bic[10], col="red", cex = 2, pch = 20)

coef(the_big_game.fwd, 10)

```


The model that used Cp as a measures had 19 variables and BIC had 10 variables. Both measurements failed to include ACC which we would expect to be in the model based on previous literature. Cp also had the additional variables ELU, BCV, SFA, CTH, RLS, POW, KPW, KAC, RBK, and PBK. For BIC, it had the predictor SFA incorrectly included and ACC incorrectly left out.