---
title: "Multiple Linear Regression - Example Code and Activities"
author: "Math 437 Spring 2023"
date: "2/27/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Data

We are going to be working with baseball salary data again. This time we will be using data from the Lahman database, found in the `Lahman` package, and hopefully there will be a few players you have heard of.

We're going to try to predict hitters' salary again. This is going to take a bit of setup; this code comes directly from the help files from the Lahman package.

```{r create hitters data, warning = FALSE, message = FALSE}
library(Lahman) # Still baseball data, but more accurate and more current
library(dplyr)

salaries <- Salaries %>%
  select(playerID, yearID, teamID, salary)
peopleInfo <- People %>%
  select(playerID, birthYear, birthMonth, nameLast,
         nameFirst, bats)
batting <- battingStats() %>% 
  left_join(salaries, 
            by =c("playerID", "yearID", "teamID")) %>%
  left_join(peopleInfo, by = "playerID") %>%
  mutate(age = yearID - birthYear - 
           1L *(birthMonth >= 10)) %>%
  arrange(playerID, yearID, stint)

```

Now let's filter so we just have the 2016 data (the latest year salary information is available) and put in a few extra conditions so that we're pretty sure we only have full-time hitters, not part-time players, injured players, or pitchers.

```{r batting-2016}
batting_2016 <- batting %>% filter(yearID == 2016,
                                   !is.na(salary),
                                   G >= 100, AB >= 200
                                   ) %>%
  mutate(salary = salary/1000) # salary in thousands
```

## Our First Model

Let's try to predict the salary based on the number of home runs and the number of walks.

```{r model-1}
lm1 <- lm(salary ~ HR + BB, data = batting_2016)
summary(lm1)
```

1. How do we interpret each number in the Estimate column?
Intercept: 
We predict the salary will be 855k dollars when both HR & BB are 0.
We predict the salary will increase by about $122k when HR increases by 1 & BB
remains constant.
We predict the salary will increase by about $71k when BB increases by 1 & HR
remains constant.


### The ANOVA Test

We can do an ANOVA test for the significance of the full model:

```{r ANOVA-1}
F.stat <- summary(lm1)$fstatistic  # value, numdf, dendf: F(numdf, dendf) = value
F.stat
```

```{r ANOVA-2}
# wrapping the assignment in () will also output to console
(p.value <- pf(F.stat[1], F.stat[2], F.stat[3], lower.tail = FALSE))
```

Notice that the p-value we got with the `pf` function is the same as what came out of the `summary` function!

We can also do partial ANOVA tests. R actually will run a partial ANOVA test for the significance of adding each variable to the model one-at-a-time:

```{r ANOVA-3}
anova(lm1)
summary(lm1)
```

Notice that the p-value for `BB` is the same as was output in the summary, but the p-value for `HR` is different. It turns out that partial ANOVA is very sensitive to the order that you input the terms into the model:

```{r ANOVA-4}
lm2 <- lm(salary ~ BB + HR, data = batting_2016)
anova(lm2)
summary(lm1)
```

1. Why is this?
lm1 ANOVA for HR: H_0: null model mean_salary = beta_0
                  H_a: mean_salary = beta_0 + beta_1*HR
                  
lm2 ANOVA for BB: H_0: null model mean_salary = beta_0 + beta_1*BB
                  H_a: mean_salary = beta_0 + beta_1*BB + beta_2*HR 

### R-Squared

```{r r-squared}
summary(lm1)$r.squared
```

1. How do we interpret the R-squared value of 0.128?
$R^2$ = 0.128 means our model can explain 12.8% of the variation in salary (our y-variable).

Notice that this is different from:

```{r adj r-squared}
summary(lm1)$adj.r.squared
```

We will talk about adjusted R-squared after we get back from spring break.

When we have more than one variable, we can increase $R^2$ by adding more
variables - even if they are more-or-less useless.

### Parameter Confidence Intervals

Confidence intervals for each parameter in the model use the `confint` function:

```{r confint}
confint(lm1, level = 0.99) 
```

If you want the confidence interval for the slope corresponding to a specific predictor, you can either specify the row in the summary output or the name of the predictor:

```{r confint-2}
confint(lm1, parm = "HR")
confint(lm1, parm = 2)
```

Notice that we get 95% CI by default.

### Prediction

To predict the salary for new players, we use the `predict` function.

```{r pred-1}
predict(lm1) %>% head(10) # first 10 predictions
```

When we don't pass in a `newdata` argument, it just predicts for each row in the training set. We should typically pass in the holdout set for `newdata`. Here we don't have a holdout set, so I just create some fake data:

```{r pred-2}
new.df <- tibble(HR = c(2, 10, 15), BB = c(30, 50, 60))

predict(lm1, newdata = new.df)
```

It's much easier to read out if we add the predictions to the data frame. The `broom` package makes this really easy:

```{r pred with broom}
broom::augment(lm1,
               newdata = new.df)
```

We can get confidence intervals for $\mu_{Y}$ and prediction intervals for $y$ by adding additional arguments to the `predict` function.

```{r confint and predint}
predict(lm1, newdata = new.df, interval = "confidence", level = 0.95)
predict(lm1, newdata = new.df, interval = "prediction", level = 0.95)
```

1. What's the difference between these intervals?
The prediction interval includes an error term $\epsilon$ making the interval
wider. The confidence interval only concerns the estimated mean of the salary.

We can also use `augment` to make our life much easier:

```{r pred with broom CI}
broom::augment(lm1,
               newdata = new.df,
               interval = "confidence")
```

2. How do we interpret the 95% confidence interval in row 1?

We estimate with 95% confidence that population average salary of a baseball player
with 2 homeruns and 30 BB will be between \$1710k and $4738k.

## Checking Model Assumptions

The workhorse plot is our residual vs. fit plot:

```{r residual plot}
plot(lm1, which = 1)
```

1. What patterns do you notice in this residual plot?
A set of data points that is almost creating its own line. This is occurring below the 0 line but not above which implies "fanning"/heteroskedasticity where the variation in residuals depends on y^hat. The average between residuals and fitted values is consistently below 0 when it should be "hovering" around 0.

To check normality we can create a normal q-q plot. The `qqnorm` plot will do this with any dataset, but if we want specifically a q-q plot of the residuals, we can use:

```{r qqplot}
plot(lm1, which = 2)
```

2. What does this plot tell us? Does this confirm any suspicions from the residual plot?

On the y-axis we have the z-scores for the residuals. On the x-axis, we have the z-score corresponding to ECDF proportions. The shape suggests skewed right, z-scores too low in the middle and too high on the extremes. Vice versa suggests skewed left.

If we're unsure about heteroskedasticity, we can use the scale-location plot to help us:

```{r scale-location}
plot(lm1, which = 3)
```

Three other plots help us identify potential outliers and high-leverage points:

```{r cook and leverage}
plot(lm1, which = 4)  # Cook's distance
plot(lm1, which = 5)  # residual vs. leverage
plot(lm1, which = 6)  # Cook's distance vs leverage
```

3. Which players get consistently identified as potential outliers? First find the row numbers in the plots above, then fill in the code chunk below to identify the players corresponding to those row numbers:

```{r check outliers}
batting_2016 %>% 
  slice(26, 29, 134) %>% # fill in slice() with the identified row numbers
  select(nameFirst, nameLast, HR, BB, salary, age)
```

Are there any players with crazy-high leverage?

```{r check leverage-1}
hist(hatvalues(lm1))
```

We expect the leverage to be right-skewed, but I don't see any outliers, so probably not anything super-concerning. The highest leverage player is...

```{r check leverage-2}
batting_2016 %>%
  slice(which.max(hatvalues(lm1))) %>%
  select(nameFirst, nameLast, HR, BB, salary, age)
```


## Collinearity and Multicollinearity

To check for collinearity, we can obtain the correlation matrix:

```{r check collinearity}
with(batting_2016, cor(HR, BB))
# alternatively batting_2016 %>% dplyr::select(HR, BB) %>% cor()
```
with() attaches data and detaches immediately after code has been run

The correlation here is about 0.5. Is this a big deal? Hard to tell. We can check the variance inflation factor (vif):


```{r vif warning = FALSE, message=FALSE}
library(car)
vif(lm1)
```

A vif of 1.35 is not that bad. This suggests that even though we have some collinearity, it will not massively affect our coefficient estimates or standard errors.

When we have more than 2 predictors in the model, we have to also watch out for multicollinearity. For an example of extreme collinearity and multicollinearity, we will use three predictors: slugging percentage, on-base percentage, and OPS: 

```{r correlation matrix}
cor(batting_2016 %>% select(SlugPct, OBP, OPS))
```

Notice that any model with these predictors could be massively affected by collinearity:

```{r collinearity}
lm3 <- lm(salary ~ SlugPct + OPS, data = batting_2016)
summary(lm3)

vif(lm3)
```

Yep, a variance inflation factor of 10.6 suggests major collinearity issues!

F-statistic: 7.036 on 2 and 225 DF,  p-value: 0.001086 : p-value implies
something in the model is making a difference from the model with no predictors
so the model has a better fit to the data. However, neither variable is being
marked as significant in this model BECAUSE of their collinearity.



But look at what happens when we try to include all three predictors:

```{r multicollinearity}
lm4 <- lm(salary ~ OBP + SlugPct + OPS, data = batting_2016)
summary(lm4) # Can't even fit the model!
```

What is going on here?
1 not defined, OPS, because OPS is a linear combination of OBP and SlugPct


The `vif` function throws an error that sheds some light on the issue:

```{r vif with multicollinearity, eval = F}
vif(lm4)
```

## Indicator Variables in Multiple Linear Regression

Let's add a single indicator variable for the league the player is in:

```{r dummy-1}
lm.dummy <- lm(salary ~ lgID, data = batting_2016)
summary(lm.dummy)
```

1. What is our reference level here?
```{r}
unique(batting_2016["lgID"])
```


2. What is our least-squares regression equation?
    Salary_hat = 7364.8 + (-2071.1)*(lgID = NL)

3. What is the predicted salary for a player in the American League? In the National League?
    For AL: Salary^hat = 7364.8 + (-2071.1)*(0) = 7364.8
    For NL: Salary^hat = 7364.8 + (-2071.1)*(1) = 5293.7
  
4. How do we interpret the slope in this model?
    When a player switches from AL to NL, the expected (predicted) salary drops
    by 2071k.

By default, the reference level is the first alphabetically. To use a different reference level, we need to use the `relevel` function. However, this function only works on factor variables:

```{r relevel}
batting_2016a <- batting_2016 %>% mutate(league = as.factor(lgID))
batting_2016a$league <- relevel(batting_2016a$league, ref = "NL")

lm.dummy2 <- lm(salary ~ league, data = batting_2016a)
summary(lm.dummy2)
```

Notice what happens with the coefficients:

```{r dummy coefficients}
coef(lm.dummy)
coef(lm.dummy2)
```

### Multiple Indicator Variables

Let's subset to just the 5 NL West teams:

```{r multiple indicator variables}
nlwest <- batting_2016 %>% filter(teamID %in% c("LAN", "SFN", "SDN", "COL", "ARI"))
lm.dummy3 <- lm(salary ~ teamID, data = nlwest)
summary(lm.dummy3)
```

1. What is our reference level here?
    Team ARI
2. What is our least-squares regression equation?
    Salary^hat = 2713 + 2479(teamID = COL) + 3309(teamID = LAN)
                  + 1134(teamID = SDN) + 6333(teamID = SFN)

3. How do we interpret the slope corresponding to `LAN` in this model?
    When a player switches team from ARI to LAN, the predicted salary increases
    by 3309k.

Notice that the F-statistic and p-value for this model also test $H_0:$ the population mean salary is the same for all 5 teams. Compare to the one-way ANOVA:

```{r lm vs anova}
# One-way ANOVA
oneway.test(salary ~ teamID, data = nlwest, var.equal = TRUE)
```

## Interaction Effects

An interaction effect is coded with a `:`

```{r interaction-1}
lm.interaction <- lm(salary ~ HR + BB + HR:BB, data = nlwest)
summary(lm.interaction)
```

1. What is our least-squares regression equation?
    Salary^hat = -4405.8 + 644.1(HR) + 219.9(BB) - 12.7(HR)(BB)

2. How do we interpret the slope corresponding to `HR` in this model?
    

3. What does it mean for the interaction term to have a negative slope?
    The salary per HR decreases as BB increases.
    
When BB = 1 vs BB = 0, the slope and the intercept both change rather than just
the slope.
BB = 0: Salary^hat = -4405.8 + 644.1(HR)

BB = 1: Salary^hat = -4405.8 + 644.1(HR) + 219.9 - 12.7(HR) 
                   = -4185.9 + 631.4(HR)

Notice that all three slopes are significant at the 5% significance level, but the overall model is not! Once we start adding interaction effects, hypothesis tests start getting a bit wonky due to collinearity issues:

```{r collinearity interaction}
cor(model.matrix(lm.interaction)[,-1]) # 1st column is intercept
```

We know that adding another term to the model will make the R-squared increase. 

```{r r-squared comparison}
lm.no_interaction <- lm(salary ~ HR + BB, data = nlwest)

summary(lm.no_interaction)$r.squared
summary(lm.interaction)$r.squared
```

Is this a significant increase?

```{r anova for interaction term}
anova(lm.interaction)
```
Yes.


### Including Main and Interaction Effects

`*` is a shorcut for combining `+` and `:`, therefore to include both main and interaction effects of HR and BB:

```{r main and interaction}
lm.equivalent <- lm(salary ~ HR * BB, data = nlwest)

summary(lm.interaction)
summary(lm.equivalent)
```

### Looking for Interaction Effects

Remember that we can look at the combined effect of a quantitative and categorical predictor on the quantitative response by adding colors on a scatterplot:

```{r interaction plot 1, warning = FALSE, message = FALSE}
library(ggplot2)
interact_plot <- ggplot(batting_2016a, aes(x = HR, y = salary)) +
  geom_point(aes(color = league)) +  # color-code points
  geom_smooth(aes(color = league), method = "lm", se = FALSE)  # add a regression line for each group
print(interact_plot)
```

1. Does there appear to be an interaction effect between `league` and `HR`? Why or why not?
    Yes, because the lines are not parallel so there is possibly an interaction
    that has a significant effect on the model.

Here's the linear regression model:

```{r interaction model 2}
lm.interaction2a <- lm(salary ~ HR * league, data = batting_2016a)
summary(lm.interaction2a)
```
For AL: Salary^hat = 2991.7 + 142.2(HR) + 79.7(1) + 72.65(HR)(1) 
                   = 3071.41 + 218.48(HR)
                   
For NL: Salary^hat = 2991.7 + 142.2(HR) + 79.71(0) + 72.65(HR)(0)
                   = 2991.7 + 142.2(HR)


2. How do we interpret the slopes corresponding to each main effect?
    For every homerun a NL player has, their salary is predicted to increase
    by 142.2k. For every NL player that switches to 

3. What would be the equation for predicting salary from HR looking only at the AL? Looking only at the NL?

We can change the reference level:

```{r change ref level}
batting_2016a$league <- relevel(as.factor(batting_2016a$league), ref = "AL")

lm.interaction2b <- lm(salary ~ HR * league, data = batting_2016a)
summary(lm.interaction2b)
```

4. According to this summary, what would be the equation for predicting salary from HR looking only at the AL? Looking only at the NL?

In base R, we can use `interaction.plot`, but I think that the scatterplot is much easier to read:

```{r interaction.plot}
with(batting_2016a, interaction.plot(x.factor = HR, trace.factor = league, response = salary))
```
