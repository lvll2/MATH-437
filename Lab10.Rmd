---
title: 'Lab Assignment #10'
author: "Math 437 - Modern Data Analysis"
date: "Due April 19, 2023"
output: pdf_document
---

# Instructions

The purpose of this lab is to introduce tree-based methods for classification and regression.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries and data, message = FALSE, warning = FALSE}
library(ISLR2)
library(dplyr)
library(rpart)
library(randomForest)
library(gbm)
library(insuranceData)
library(tidymodels)
library(vip)
# if you use ranger later you should include it here
# if you use xgboost later you may want to include xgboost and/or tidymodels packages
```

This lab assignment is worth a total of **15 points**.

# Problem 1: Book Code

## Part a (Code: 0.5 pts; Explanation: 2.5 pts)

In past years, students have complained that the tree package wasn't running on their version of R. So instead of doing Labs 8.3.1 and 8.3.2 we will run similar code using the `rpart` package.

Run each of the following code chunks. When prompted, answer the questions. You may find reading the text in Labs 8.3.1 and 8.3.2 to be useful.

```{r Carseats High}
Carseats2 <- Carseats %>% mutate(High = if_else(Sales <= 8, "No", "Yes"))
```

```{r Carseats tree}
tree.carseats <- rpart(High ~. - Sales, data = Carseats2)
summary(tree.carseats)
```

```{r plot tree}
plot(tree.carseats, uniform = TRUE, ylim = c(0, 1.2))
text(tree.carseats, pretty = 0)
```

1. Briefly explain how to interpret this tree.

If the condition at the root is met we go to the left, if not we go to the right down the tree and repeat for subsequent conditions until we reach the terminal node which will be our prediction for that observation. So for example, in this case, if the shelving location is good, we go to the right and if the price is greater than or equal to 142, we predict No.

```{r print tree}
tree.carseats
```

2. What do the numbers in the parentheses mean? What do the stars mean?

The stars mean that that branch leads to a terminal node. The numbers in the parentheses represent the probabilities of being in each class (Yes No) for that branch.

```{r tree model}
set.seed(2)
train <- sample(1:nrow(Carseats2), 200)
Carseats.test <- Carseats2[-train,]

tree.carseats <- rpart(High ~ . - Sales, data = Carseats2, subset = train, control = rpart.control(xval = 10))
names(tree.carseats)
```

3. What does the argument `xval = 10` mean?
The argument means we want 10 cross-validations to be performed.

```{r tree prediction}
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, Carseats.test$High, dnn = c("Predicted", "Actual"))
```

```{r Boston}
set.seed(1)
n <- nrow(Boston)
train <- sample(1:n, n/2)
tree.boston <- rpart(medv~., Boston, subset = train, control = rpart.control(xval = 10))
```

```{r Boston regression}
plot(tree.boston, uniform = TRUE, ylim = c(0, 1.4))
text(tree.boston, pretty = 0)
```

4. Briefly explain how to interpret this tree.
We are interperting this tree in the same fashion as the earlier tree so for example, if our observation has rm < 6.96 we go to the left of the root and if lstat >= 14.4 we go left again and if crim < 11.49 we go right down the tree and if age >= 93.95 we predict the median value to be 14.43.

```{r Boston prediction}
yhat <- predict(tree.boston, Boston[-train,])
boston.test <- Boston$medv[-train]
plot(yhat, boston.test)
abline(0,1)
mean((yhat - boston.test)^2)
```

5. Why do we get this pattern - a bunch of vertical lines?
We get this pattern because we only have 7 possible values of $y$ that correspond to the 7 terminal nodes of our decision tree.

### Part b (Code: 1 pt)

Run the code in ISLR Labs 8.3.3 and 8.3.4. Put each chunk from the textbook in its own chunk.

### Part c (Explanation: 1 pt)

Consider the following statement: the second decision tree that is fit does not depend on the first decision tree that is fit. Is that statement TRUE or FALSE for the random forest algorithm? What about for bootsted trees? Explain your reasoning.

## Problem 2: Auto Insurance Claims

The `AutoBi` dataset in the `insuranceData` package contains information about a sample of 1,340 automobile insurance claims from 2002.

The response variable here is `LOSS`, the total economic loss (in thousands). We take the base 10 log of the claimed loss to make the regression a bit easier, and transform several predictors, to create the `AutoBi2` dataset.

```{r insurance fix}
data(AutoBi) # Auto insurance claim dataset

AutoBi2 <- AutoBi %>% transmute(
  attorney = if_else(ATTORNEY == 1, "yes", "no"),
  gender = if_else(CLMSEX == 1, "male", "female"),
  marital = case_when(MARITAL == 1 ~ "married",
                      MARITAL == 2 ~ "single",
                      MARITAL == 3 ~ "widowed",
                      MARITAL == 4 ~ "divorced",
                      TRUE ~ NA_character_),
  driver_insured = if_else(CLMINSUR == 1, "yes", "no"),
  seatbelt = if_else(SEATBELT == 1, "yes", "no"),
  age = CLMAGE,
  log_loss = log10(1000*LOSS) # log 10 of claimed loss in the claim
) %>% filter(!is.na(gender), !is.na(marital), !is.na(driver_insured),
                              !is.na(seatbelt), !is.na(age))
```

### Part a (Code: 1 pt)

Divide the `AutoBi2` dataset into a training set and a test set. The test set should contain approximately 25% of the original dataset.

```{r}
set.seed(11249)
auto_split <- initial_split(AutoBi2, prop = 0.75)
auto_train <- training(auto_split)
auto_test <- testing(auto_split)
```


### Part b (Code: 3 pts)

Using tidymodels with either the `randomForest` or `ranger` package, fit a random forest model on the training set. Make sure to use `set_args` to tell tidymodels you want to use permutation-based importance (you will need it for part d). Tune the value of `mtry` using cross-validation. Use values from 2 to 6 (6 = bagging). 

Obtain predictions on the test set and compute the estimated test RMSE. 

```{r rf-tidy model}
rfR_model <- rand_forest(mode = "regression", engine = "ranger") %>%
  set_args(seed = 395,
           importance = "permutation",
           mtry = tune())
# you can use .cols() or .preds(); .preds() works before dummy variables are created
# and .cols() works afterwards

rfR_recipe <- recipe(
  log_loss ~ ., # response ~ predictors
  data = auto_train
)

rfR_wflow <- workflow() %>%
  add_model(rfR_model) %>%
  add_recipe(rfR_recipe) # same recipe as earlier
```

```{r tune model kfold rfR}
set.seed(1332)
auto_kfold <- vfold_cv(auto_train, v = 5, repeats = 3) 

# I'm sure there's a better way, but this works
n_predictors <- sum(rfR_recipe$var_info$role == "predictor")
manual_grid <- expand.grid(mtry = seq(2, n_predictors))
# maybe don't search over the entire grid if you have a ton of predictors
rfR_tune1 <- tune_grid(rfR_model, 
                      rfR_recipe, 
                      resamples = auto_kfold, 
                      grid = manual_grid)
```

```{r select best rf}
rfR_best <- select_by_one_std_err(
  rfR_tune1,
  metric = "rmse",
  mtry
)
rfR_best
```

```{r fit rf-tidy model}
rfR_wflow_final <- finalize_workflow(rfR_wflow, parameters = rfR_best) 

rfR_fit <- fit(rfR_wflow_final, data = auto_train)
rfR_fit
```

```{r augment baggingR fit}
predictions_rfR <- broom::augment(rfR_fit, new_data = auto_test)
predictions_rfR %>% dplyr::select(
  log_loss, .pred
)
rmse(predictions_rfR, truth = log_loss, estimate = .pred)
```

### Part c (Code and Explanation: 2 pts)

Obtain the out-of-bag prediction error on the training set.

Compare MSE on the test set to the out-of-bag prediction error on the training set.
```{r rfR get OOB MSE}
rfR_engine <- rfR_fit %>% extract_fit_engine()
rfR_fit %>% extract_fit_engine() %>% pluck("prediction.error")
```

### Part d (Code: 1 pt; Explanation: 1 pt)

Obtain estimates of variable importance (use the `%IncMSE` column if you are using `randomForest`). Which predictor variables have the largest importance and are thus the most useful for making the predictions? Which predictor variables have negative importance, suggesting that randomly permuting that variable actually *decreases* MSE?

```{r vip rfR with extra stuff}
vip(rfR_engine, scale = TRUE)
```

### Part e (Code: 2 pts)

Using tidymodels with the `xgboost` package, fit a gradient-boosted tree model on the training set. There are 8 different tuning parameters: we are just going to tune the 3 discussed in lecture: the number of trees to fit (`trees`), interaction depth (`tree_depth`), and learning rate (`learn_rate`). In your cross-validation, search over the following grid: 10, 25, 50 and 100 for `trees`, 1 and 2 for `tree_depth`, and a logarithmically spaced grid from 0.01 to 1 for `learn_rate`.

Obtain predictions on the test set and compute the estimated test RMSE.

```{r xgboost R}
xgboostR_model <- boost_tree(mode = "regression", engine = "xgboost",
                            trees = tune(), tree_depth = tune(),
                            learn_rate = tune())

xgboostR_recipe <- recipe(
log_loss ~ ., # response ~ predictors
  data = auto_train
) %>%
  step_dummy(all_nominal_predictors())

xgboostR_wflow <- workflow() %>%
  add_model(xgboostR_model) %>%
  add_recipe(xgboostR_recipe)
```

```{r tune parameters xgboostR}
set.seed(437)
auto_kfold <- vfold_cv(auto_train, v = 5, repeats = 3) 

param_grid <- expand.grid(trees = c(10,25,50,100), tree_depth = c(1,2),learn_rate = 10^seq(.01, 1))

xgboostR_tune <- tune_grid(xgboostR_model, 
                      xgboostR_recipe, 
                      resamples = auto_kfold,
                      metrics = metric_set(rmse),
                      grid = param_grid) # search over 20 possible combinations of the three parameters
```

```{r select best}
xgboostR_tune %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean)
```

```{r finalize xgboostR}
xgboostR_best <- select_by_one_std_err(xgboostR_tune, 
                             metric = "rmse", 
                             tree_depth, trees, desc(learn_rate))
xgboostR_wflow_final <- finalize_workflow(xgboostR_wflow, parameters = xgboostR_best) 
```

```{r fit and predict xgboostR}
xgboostR_fit <- fit(xgboostR_wflow_final, data = auto_train)
xgboostR_predict <- augment(xgboostR_fit, new_data = auto_test)
xgboostR_predict %>% 
  dplyr::select(log_loss, .pred) %>%
  dplyr::slice(1:10) # must use dplyr::slice because there is also a slice function in xgboost
rmse(xgboostR_predict, truth = log_loss, estimate = .pred)
```